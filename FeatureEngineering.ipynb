{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP97QPosGhD2ANMBJ9wem9O"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CybdSLP5rQ5t"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import spacy\n",
        "from gensim.models import Word2Vec\n",
        "from imblearn.combine import SMOTETomek\n",
        "import nltk\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "from gensim.models import LdaModel\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay, precision_recall_curve, auc\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZbBJCZG3_OB",
        "outputId": "d6b80253-96fb-40d7-e198-6879a7cf72ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Fall 2024/SML 312/Final Project/data'\n",
        "\n",
        "train_df = pd.read_csv(file_path + '/train.csv')\n",
        "val_df = pd.read_csv(file_path + '/val.csv')\n",
        "test_df = pd.read_csv(file_path + '/test.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dr6wfwH3rWta",
        "outputId": "9cfae2d3-3ae5-4200-cfd6-2f154fe3f96c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['clean_prompt'] = train_df['clean_prompt'].fillna('')\n",
        "val_df['clean_prompt'] = val_df['clean_prompt'].fillna('')\n",
        "test_df['clean_prompt'] = test_df['clean_prompt'].fillna('')"
      ],
      "metadata": {
        "id": "BcQdk5i0roAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for viewing evaluation metrics\n",
        "\n",
        "def evaluation_metrics(title, y_pred_prob, y_true, threshold=0.5, plots=True):\n",
        "  y_pred = (y_pred_prob >= threshold).astype(int)\n",
        "\n",
        "  precision, recall, thresholds = precision_recall_curve(y_true, y_pred_prob)\n",
        "  pr_auc = auc(recall, precision)\n",
        "\n",
        "  accuracy, recall, _f1_score = accuracy_score(y_true, y_pred), recall_score(y_true, y_pred), f1_score(y_true, y_pred)\n",
        "\n",
        "  results = pd.DataFrame({\n",
        "    'Model': [title],\n",
        "    'Accuracy': [accuracy],\n",
        "    'Recall': [recall],\n",
        "    'F1-Score': [_f1_score],\n",
        "    'Precision-Recall AUC': [pr_auc]\n",
        "  })\n",
        "\n",
        "  print(f\"=== {title} Classification Report ===\")\n",
        "  print(results)\n",
        "\n",
        "  if plots:\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "    disp = ConfusionMatrixDisplay(conf_matrix, display_labels=['Benign', 'Jailbreak'])\n",
        "    disp.plot(ax=axes[0], cmap=plt.cm.Blues)\n",
        "    axes[0].set_title(title + ' Confusion Matrix')\n",
        "\n",
        "    precision, recall, thresholds = precision_recall_curve(y_true, y_pred_prob)\n",
        "\n",
        "    axes[1].plot(recall, precision, color='blue', lw=2, label=f'PR curve (area = {pr_auc:.2f})')\n",
        "    axes[1].set_xlabel('Recall')\n",
        "    axes[1].set_ylabel('Precision')\n",
        "    axes[1].set_title(title + ' Precision-Recall Curve')\n",
        "    axes[1].legend(loc=\"lower left\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return (results, fig)\n",
        "\n",
        "  else:\n",
        "    return (results, None)"
      ],
      "metadata": {
        "id": "2iRhN7d7j7SD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# max_features parameter in TF-IDF controls how many of most common features (words) to include\n",
        "# Here, I experiment with various max_features values using unigrams only (with Logistic Regression model to demonstrate results)\n",
        "\n",
        "max_features_vals = [1000, 2000, 3000, 5000, 8000, 10000]\n",
        "\n",
        "for max_features in max_features_vals:\n",
        "  tfidf_vectorizer = TfidfVectorizer(max_features=max_features)\n",
        "  log_reg = LogisticRegression(max_iter=10000, class_weight='balanced')\n",
        "\n",
        "  train_features = tfidf_vectorizer.fit_transform(train_df['clean_prompt'])\n",
        "  val_features = tfidf_vectorizer.transform(val_df['clean_prompt'])\n",
        "\n",
        "  log_reg.fit(train_features, train_df['jailbreak'])\n",
        "  y_pred_prob = log_reg.predict_proba(val_features)[:, 1]\n",
        "  results, _ = evaluation_metrics(f'Logistic Regression (TF-IDF max_features = {max_features})', y_pred_prob, val_df['jailbreak'], plots=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iR550ZL1jdGX",
        "outputId": "b0a11c68-9952-41cb-f17c-01cd47581967"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Logistic Regression (TF-IDF max_features = 1000) Classification Report ===\n",
            "                                              Model  Accuracy    Recall  \\\n",
            "0  Logistic Regression (TF-IDF max_features = 1000)  0.873844  0.758865   \n",
            "\n",
            "   F1-Score  Precision-Recall AUC  \n",
            "0  0.528395              0.596183  \n",
            "=== Logistic Regression (TF-IDF max_features = 2000) Classification Report ===\n",
            "                                              Model  Accuracy    Recall  \\\n",
            "0  Logistic Regression (TF-IDF max_features = 2000)  0.891017  0.780142   \n",
            "\n",
            "   F1-Score  Precision-Recall AUC  \n",
            "0  0.571429              0.596443  \n",
            "=== Logistic Regression (TF-IDF max_features = 3000) Classification Report ===\n",
            "                                              Model  Accuracy    Recall  \\\n",
            "0  Logistic Regression (TF-IDF max_features = 3000)  0.900925  0.822695   \n",
            "\n",
            "   F1-Score  Precision-Recall AUC  \n",
            "0   0.60733              0.604239  \n",
            "=== Logistic Regression (TF-IDF max_features = 5000) Classification Report ===\n",
            "                                              Model  Accuracy    Recall  \\\n",
            "0  Logistic Regression (TF-IDF max_features = 5000)  0.910832  0.794326   \n",
            "\n",
            "   F1-Score  Precision-Recall AUC  \n",
            "0  0.623955              0.605961  \n",
            "=== Logistic Regression (TF-IDF max_features = 8000) Classification Report ===\n",
            "                                              Model  Accuracy    Recall  \\\n",
            "0  Logistic Regression (TF-IDF max_features = 8000)  0.913474  0.787234   \n",
            "\n",
            "   F1-Score  Precision-Recall AUC  \n",
            "0  0.628895              0.606362  \n",
            "=== Logistic Regression (TF-IDF max_features = 10000) Classification Report ===\n",
            "                                               Model  Accuracy    Recall  \\\n",
            "0  Logistic Regression (TF-IDF max_features = 10000)  0.910832  0.780142   \n",
            "\n",
            "   F1-Score  Precision-Recall AUC  \n",
            "0  0.619718              0.616645  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since I prioritize recall (misclassified jailbreak prompts have most serious consequences), the performance is best when using TF-IDF with max_features = 3000."
      ],
      "metadata": {
        "id": "N35IB2OMkYvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, I experiment with various max_features values using unigrams and bigrams (again with Logistic Regression model to demonstrate results)\n",
        "\n",
        "max_features_vals = [2000, 4000, 6000, 8000, 10000, 12000]\n",
        "\n",
        "for max_features in max_features_vals:\n",
        "  tfidf_vectorizer = TfidfVectorizer(max_features=max_features, ngram_range=(1, 2))\n",
        "  log_reg = LogisticRegression(max_iter=10000, class_weight='balanced')\n",
        "\n",
        "  train_features = tfidf_vectorizer.fit_transform(train_df['clean_prompt'])\n",
        "  val_features = tfidf_vectorizer.transform(val_df['clean_prompt'])\n",
        "\n",
        "  log_reg.fit(train_features, train_df['jailbreak'])\n",
        "  y_pred_prob = log_reg.predict_proba(val_features)[:, 1]\n",
        "  results, _ = evaluation_metrics(f'Logistic Regression (TF-IDF max_features = {max_features}, bigrams included)', y_pred_prob, val_df['jailbreak'], plots=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACycy1_wkH9K",
        "outputId": "68783b49-c362-435e-81c7-1bd81a8b5f15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Logistic Regression (TF-IDF max_features = 2000, bigrams included) Classification Report ===\n",
            "                                               Model  Accuracy    Recall  \\\n",
            "0  Logistic Regression (TF-IDF max_features = 200...  0.887715  0.780142   \n",
            "\n",
            "   F1-Score  Precision-Recall AUC  \n",
            "0  0.564103              0.619026  \n",
            "=== Logistic Regression (TF-IDF max_features = 4000, bigrams included) Classification Report ===\n",
            "                                               Model  Accuracy    Recall  \\\n",
            "0  Logistic Regression (TF-IDF max_features = 400...  0.904227  0.808511   \n",
            "\n",
            "   F1-Score  Precision-Recall AUC  \n",
            "0   0.61126              0.621519  \n",
            "=== Logistic Regression (TF-IDF max_features = 6000, bigrams included) Classification Report ===\n",
            "                                               Model  Accuracy    Recall  \\\n",
            "0  Logistic Regression (TF-IDF max_features = 600...  0.912153  0.794326   \n",
            "\n",
            "   F1-Score  Precision-Recall AUC  \n",
            "0  0.627451              0.624427  \n",
            "=== Logistic Regression (TF-IDF max_features = 8000, bigrams included) Classification Report ===\n",
            "                                               Model  Accuracy    Recall  \\\n",
            "0  Logistic Regression (TF-IDF max_features = 800...  0.912153  0.780142   \n",
            "\n",
            "   F1-Score  Precision-Recall AUC  \n",
            "0  0.623229               0.62304  \n",
            "=== Logistic Regression (TF-IDF max_features = 10000, bigrams included) Classification Report ===\n",
            "                                               Model  Accuracy    Recall  \\\n",
            "0  Logistic Regression (TF-IDF max_features = 100...  0.912153  0.787234   \n",
            "\n",
            "   F1-Score  Precision-Recall AUC  \n",
            "0  0.625352              0.625094  \n",
            "=== Logistic Regression (TF-IDF max_features = 12000, bigrams included) Classification Report ===\n",
            "                                               Model  Accuracy    Recall  \\\n",
            "0  Logistic Regression (TF-IDF max_features = 120...  0.911493  0.765957   \n",
            "\n",
            "   F1-Score  Precision-Recall AUC  \n",
            "0  0.617143              0.628145  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, since recall is prioritized, the inclusion of bigrams does not appear to improve model performance. Therefore, I use TF-IDF with unigrams only."
      ],
      "metadata": {
        "id": "Mr-CyHwTka_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF\n",
        "# 3000 features and unigrams only yielded best results (shown above)\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=3000)\n",
        "\n",
        "tfidf_train = tfidf_vectorizer.fit_transform(train_df['clean_prompt'])\n",
        "tfidf_val = tfidf_vectorizer.transform(val_df['clean_prompt'])\n",
        "tfidf_test = tfidf_vectorizer.transform(test_df['clean_prompt'])\n",
        "\n",
        "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Source: https://www.deepwizai.com/projects/how-to-correctly-use-tf-idf-with-imbalanced-data"
      ],
      "metadata": {
        "id": "kK1lCBfDsBTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tfidf_train.shape)\n",
        "print(tfidf_val.shape)\n",
        "print(tfidf_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dyve1qoK3OZf",
        "outputId": "b2b591b9-42bc-43f0-f63e-5a7312f2d24b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(12112, 3000)\n",
            "(1514, 3000)\n",
            "(1514, 3000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Word2Vec (CBoW architecture)\n",
        "\n",
        "def compute_mean_embedding(model, tokenized_prompts):\n",
        "    embeddings = [\n",
        "        model.wv[word] for word in tokenized_prompts if word in model.wv\n",
        "    ]\n",
        "    if len(embeddings) > 0:\n",
        "        return np.mean(embeddings, axis=0)\n",
        "    else:\n",
        "        return np.zeros(model.vector_size)\n",
        "\n",
        "def precompute_embeddings(model, clean_prompts):\n",
        "    tokenized_prompts = clean_prompts.apply(word_tokenize).tolist()\n",
        "    return np.array([compute_mean_embedding(model, tokens) for tokens in tokenized_prompts])\n",
        "\n",
        "tokenized_prompts_train = train_df['clean_prompt'].apply(word_tokenize).tolist()\n",
        "cbow_model = Word2Vec(tokenized_prompts_train, vector_size=100, window=5, min_count=1, workers=4, sg=0)\n",
        "\n",
        "cbow_embeddings_train = precompute_embeddings(cbow_model, train_df['clean_prompt'])\n",
        "cbow_embeddings_val = precompute_embeddings(cbow_model, val_df['clean_prompt'])\n",
        "cbow_embeddings_test = precompute_embeddings(cbow_model, test_df['clean_prompt'])\n",
        "\n",
        "# Source: https://www.analyticsvidhya.com/blog/2021/07/word2vec-for-word-embeddings-a-beginners-guide/\n",
        "# Source: https://medium.com/@dilip.voleti/classification-using-word2vec-b1d79d375381"
      ],
      "metadata": {
        "id": "lbbx0AGv3cxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(cbow_embeddings_train.shape)\n",
        "print(cbow_embeddings_val.shape)\n",
        "print(cbow_embeddings_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSuy0AOp4RFB",
        "outputId": "6ceeef39-fba2-407e-a0e8-aea998cc3869"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(12112, 100)\n",
            "(1514, 100)\n",
            "(1514, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SMOTE + Tomek Links applied to Word2Vec embeddings\n",
        "\n",
        "smote_tomek = SMOTETomek(random_state=42)\n",
        "train_embeddings_resampled, train_labels_resampled = smote_tomek.fit_resample(cbow_embeddings_train, train_df['jailbreak'])\n",
        "\n",
        "print(train_embeddings_resampled.shape)\n",
        "print(train_labels_resampled.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAvbsK2C4ajT",
        "outputId": "67b94aa0-0fc4-4e89-b5b3-40b5e74042ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(21960, 100)\n",
            "(21960,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LDA Topic Modelling\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def lda(train, val, test, n_topics):\n",
        "  # Tokenize training prompts\n",
        "  train_docs = train['clean_prompt'].apply(word_tokenize)\n",
        "  train_docs = [[token for token in doc if token not in stop_words] for doc in train_docs]\n",
        "\n",
        "  # Create dictionary and corpus from training docs\n",
        "  train_dictionary = Dictionary(train_docs)\n",
        "  train_dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
        "  train_corpus = [train_dictionary.doc2bow(doc) for doc in train_docs]\n",
        "\n",
        "  lda_model = LdaModel(\n",
        "      corpus=train_corpus,\n",
        "      id2word=train_dictionary,\n",
        "      chunksize=2000,\n",
        "      passes=10,\n",
        "      alpha='auto',\n",
        "      eta='auto',\n",
        "      iterations=400,\n",
        "      num_topics=n_topics,\n",
        "      eval_every=None,\n",
        "      random_state=42\n",
        "  )\n",
        "\n",
        "  # Extract topic distributions for training set\n",
        "  train_topic_distributions = [lda_model.get_document_topics(bow, minimum_probability=0) for bow in train_corpus]\n",
        "  train_topic_matrix = np.zeros((len(train_topic_distributions), n_topics))\n",
        "  for i, dist in enumerate(train_topic_distributions):\n",
        "      for topic_id, prob in dist:\n",
        "          train_topic_matrix[i, topic_id] = prob\n",
        "\n",
        "  # Tokenize validation prompts\n",
        "  val_docs = val['clean_prompt'].apply(word_tokenize)\n",
        "  val_docs = [[token for token in doc if token not in stop_words] for doc in val_docs]\n",
        "  val_corpus = [train_dictionary.doc2bow(doc) for doc in val_docs]\n",
        "\n",
        "  # Tokenize test prompts\n",
        "  test_docs = test['clean_prompt'].apply(word_tokenize)\n",
        "  test_docs = [[token for token in doc if token not in stop_words] for doc in test_docs]\n",
        "  test_corpus = [train_dictionary.doc2bow(doc) for doc in test_docs]\n",
        "\n",
        "  # Extract topic distributions for validation set\n",
        "  val_topic_distributions = [lda_model.get_document_topics(bow, minimum_probability=0) for bow in val_corpus]\n",
        "  val_topic_matrix = np.zeros((len(val_topic_distributions), n_topics))\n",
        "  for i, dist in enumerate(val_topic_distributions):\n",
        "    for topic_id, prob in dist:\n",
        "        val_topic_matrix[i, topic_id] = prob\n",
        "\n",
        "  # Extract topic distributions for test set\n",
        "  test_topic_distributions = [lda_model.get_document_topics(bow, minimum_probability=0) for bow in test_corpus]\n",
        "  test_topic_matrix = np.zeros((len(test_topic_distributions), n_topics))\n",
        "  for i, dist in enumerate(test_topic_distributions):\n",
        "    for topic_id, prob in dist:\n",
        "        test_topic_matrix[i, topic_id] = prob\n",
        "\n",
        "  train_topic_df = pd.DataFrame(train_topic_matrix, columns=[f\"topic_{i}\" for i in range(n_topics)])\n",
        "  val_topic_df = pd.DataFrame(val_topic_matrix, columns=[f\"topic_{i}\" for i in range(n_topics)])\n",
        "  test_topic_df = pd.DataFrame(test_topic_matrix, columns=[f\"topic_{i}\" for i in range(n_topics)])\n",
        "\n",
        "  return train_topic_df, val_topic_df, test_topic_df, lda_model"
      ],
      "metadata": {
        "id": "fFjfssIke2fJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test LDA Topic Modelling with various num_topics (using Logistic Regression model to demonstrate results)\n",
        "\n",
        "num_topics = [3, 5, 10, 15, 20, 30]\n",
        "\n",
        "for n in num_topics:\n",
        "  train_lda_df, val_lda_df, test_lda_df, lda_model = lda(train_df, val_df, test_df, n)\n",
        "  train_features = train_lda_df\n",
        "  val_features = val_lda_df\n",
        "\n",
        "  log_reg = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
        "  log_reg.fit(train_features, train_df['jailbreak'])\n",
        "\n",
        "  y_pred_prob = log_reg.predict_proba(val_features)[:, 1]\n",
        "  results, _ = evaluation_metrics(f'Logistic Regression (LDA, n_topics = {n})', y_pred_prob, val_df['jailbreak'], plots=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOIRiYUFkrdU",
        "outputId": "09287a7d-cda2-47b5-e92c-6c58250ee024"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.ldamodel:updated prior is not positive\n",
            "WARNING:gensim.models.ldamodel:updated prior is not positive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Logistic Regression (LDA, n_topics = 3) Classification Report ===\n",
            "                                     Model  Accuracy    Recall  F1-Score  \\\n",
            "0  Logistic Regression (LDA, n_topics = 3)  0.804491  0.815603  0.437262   \n",
            "\n",
            "   Precision-Recall AUC  \n",
            "0              0.409431  \n",
            "=== Logistic Regression (LDA, n_topics = 5) Classification Report ===\n",
            "                                     Model  Accuracy    Recall  F1-Score  \\\n",
            "0  Logistic Regression (LDA, n_topics = 5)   0.84148  0.794326  0.482759   \n",
            "\n",
            "   Precision-Recall AUC  \n",
            "0              0.455228  \n",
            "=== Logistic Regression (LDA, n_topics = 10) Classification Report ===\n",
            "                                      Model  Accuracy    Recall  F1-Score  \\\n",
            "0  Logistic Regression (LDA, n_topics = 10)   0.84214  0.808511  0.488223   \n",
            "\n",
            "   Precision-Recall AUC  \n",
            "0              0.524227  \n",
            "=== Logistic Regression (LDA, n_topics = 15) Classification Report ===\n",
            "                                      Model  Accuracy    Recall  F1-Score  \\\n",
            "0  Logistic Regression (LDA, n_topics = 15)  0.849406  0.822695  0.504348   \n",
            "\n",
            "   Precision-Recall AUC  \n",
            "0              0.539442  \n",
            "=== Logistic Regression (LDA, n_topics = 20) Classification Report ===\n",
            "                                      Model  Accuracy    Recall  F1-Score  \\\n",
            "0  Logistic Regression (LDA, n_topics = 20)  0.873844  0.801418  0.541966   \n",
            "\n",
            "   Precision-Recall AUC  \n",
            "0              0.559543  \n",
            "=== Logistic Regression (LDA, n_topics = 30) Classification Report ===\n",
            "                                      Model  Accuracy    Recall  F1-Score  \\\n",
            "0  Logistic Regression (LDA, n_topics = 30)  0.865918  0.801418  0.526807   \n",
            "\n",
            "   Precision-Recall AUC  \n",
            "0              0.552782  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LDA Topic Modelling with 20 topics\n",
        "\n",
        "train_lda_df, val_lda_df, test_lda_df, lda_model = lda(train_df, val_df, test_df, n_topics=20)\n",
        "\n",
        "print(train_lda_df.shape)\n",
        "print(val_lda_df.shape)\n",
        "print(test_lda_df.shape)\n",
        "\n",
        "topics = lda_model.print_topics(num_words=10)\n",
        "for topic_id, topic in topics:\n",
        "    print(f\"Topic {topic_id + 1}: {topic}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqftjr-Le_TT",
        "outputId": "d464ccbf-dc75-4c51-bb60-8452630d1511"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(12112, 20)\n",
            "(1514, 20)\n",
            "(1514, 20)\n",
            "Topic 1: 0.041*\"yang\" + 0.034*\"dengan\" + 0.031*\"dan\" + 0.026*\"saya\" + 0.026*\"artikel\" + 0.021*\"kata\" + 0.018*\"anda\" + 0.016*\"judul\" + 0.013*\"menulis\" + 0.012*\"jangan\"\n",
            "Topic 2: 0.019*\"question\" + 0.015*\"term\" + 0.012*\"token\" + 0.012*\"food\" + 0.011*\"financial\" + 0.010*\"time\" + 0.009*\"system\" + 0.008*\"ask\" + 0.008*\"project\" + 0.008*\"answer\"\n",
            "Topic 3: 0.138*\"story\" + 0.053*\"bird\" + 0.047*\"chan\" + 0.027*\"gpt\" + 0.022*\"lucy\" + 0.014*\"juice\" + 0.012*\"harry\" + 0.011*\"tell\" + 0.011*\"write\" + 0.011*\"protagonist\"\n",
            "Topic 4: 0.172*\"user\" + 0.057*\"response\" + 0.033*\"write\" + 0.032*\"continue\" + 0.031*\"name\" + 0.027*\"wait\" + 0.026*\"assume\" + 0.024*\"reaction\" + 0.018*\"personality\" + 0.016*\"explicit\"\n",
            "Topic 5: 0.083*\"code\" + 0.026*\"function\" + 0.024*\"print\" + 0.020*\"documentation\" + 0.016*\"expert\" + 0.015*\"project\" + 0.015*\"programming\" + 0.014*\"variable\" + 0.013*\"prompt\" + 0.013*\"use\"\n",
            "Topic 6: 0.023*\"make\" + 0.023*\"user\" + 0.020*\"ask\" + 0.018*\"like\" + 0.018*\"answer\" + 0.016*\"want\" + 0.016*\"question\" + 0.015*\"give\" + 0.015*\"use\" + 0.013*\"need\"\n",
            "Topic 7: 0.034*\"student\" + 0.016*\"learn\" + 0.016*\"step\" + 0.013*\"lesson\" + 0.013*\"lyric\" + 0.013*\"agent\" + 0.013*\"study\" + 0.012*\"persona\" + 0.012*\"song\" + 0.010*\"subject\"\n",
            "Topic 8: 0.031*\"datum\" + 0.028*\"use\" + 0.027*\"language\" + 0.014*\"system\" + 0.010*\"value\" + 0.010*\"word\" + 0.010*\"model\" + 0.009*\"analysis\" + 0.009*\"bot\" + 0.009*\"text\"\n",
            "Topic 9: 0.020*\"product\" + 0.019*\"job\" + 0.019*\"provide\" + 0.018*\"prompt\" + 0.016*\"information\" + 0.015*\"description\" + 0.012*\"user\" + 0.012*\"business\" + 0.010*\"company\" + 0.010*\"include\"\n",
            "Topic 10: 0.034*\"character\" + 0.024*\"player\" + 0.016*\"game\" + 0.014*\"world\" + 0.008*\"story\" + 0.008*\"create\" + 0.007*\"time\" + 0.006*\"use\" + 0.006*\"base\" + 0.005*\"quest\"\n",
            "Topic 11: 0.080*\"dan\" + 0.024*\"must\" + 0.019*\"like\" + 0.019*\"anything\" + 0.016*\"act\" + 0.015*\"free\" + 0.014*\"response\" + 0.012*\"example\" + 0.012*\"content\" + 0.011*\"chatgpt\"\n",
            "Topic 12: 0.022*\"content\" + 0.021*\"topic\" + 0.019*\"post\" + 0.018*\"audience\" + 0.015*\"book\" + 0.012*\"point\" + 0.012*\"summary\" + 0.012*\"social\" + 0.011*\"medium\" + 0.011*\"idea\"\n",
            "Topic 13: 0.068*\"prompt\" + 0.035*\"image\" + 0.019*\"description\" + 0.018*\"use\" + 0.016*\"generate\" + 0.015*\"style\" + 0.014*\"art\" + 0.013*\"scene\" + 0.011*\"detailed\" + 0.011*\"detail\"\n",
            "Topic 14: 0.017*\"provide\" + 0.013*\"user\" + 0.009*\"experience\" + 0.008*\"ensure\" + 0.008*\"help\" + 0.007*\"strategy\" + 0.007*\"role\" + 0.007*\"understand\" + 0.006*\"knowledge\" + 0.006*\"insight\"\n",
            "Topic 15: 0.024*\"game\" + 0.021*\"option\" + 0.021*\"number\" + 0.015*\"list\" + 0.014*\"first\" + 0.014*\"https\" + 0.013*\"start\" + 0.012*\"time\" + 0.012*\"show\" + 0.011*\"output\"\n",
            "Topic 16: 0.008*\"like\" + 0.007*\"char\" + 0.007*\"love\" + 0.007*\"use\" + 0.006*\"get\" + 0.006*\"action\" + 0.005*\"make\" + 0.005*\"body\" + 0.005*\"eye\" + 0.005*\"character\"\n",
            "Topic 17: 0.086*\"mode\" + 0.049*\"chatgpt\" + 0.028*\"enable\" + 0.027*\"developer\" + 0.022*\"generate\" + 0.022*\"que\" + 0.018*\"para\" + 0.017*\"content\" + 0.015*\"response\" + 0.015*\"normal\"\n",
            "Topic 18: 0.055*\"write\" + 0.031*\"targetlanguage\" + 0.029*\"prompt\" + 0.028*\"content\" + 0.026*\"use\" + 0.024*\"keyword\" + 0.024*\"article\" + 0.019*\"seo\" + 0.018*\"word\" + 0.015*\"title\"\n",
            "Topic 19: 0.023*\"answer\" + 0.020*\"response\" + 0.018*\"always\" + 0.018*\"never\" + 0.016*\"say\" + 0.015*\"character\" + 0.015*\"question\" + 0.014*\"request\" + 0.013*\"chatgpt\" + 0.013*\"rule\"\n",
            "Topic 20: 0.018*\"sequence\" + 0.015*\"dream\" + 0.012*\"dark\" + 0.012*\"realm\" + 0.010*\"deep\" + 0.010*\"wisdom\" + 0.010*\"embrace\" + 0.009*\"null\" + 0.009*\"spirit\" + 0.008*\"chaos\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/Fall 2024/SML 312/Final Project/data'\n",
        "\n",
        "np.save(path + '/tfidf_train.npy', tfidf_train.toarray())\n",
        "np.save(path + '/tfidf_val.npy', tfidf_val.toarray())\n",
        "np.save(path + '/tfidf_test.npy', tfidf_test.toarray())\n",
        "\n",
        "np.save(path + '/tfidf_feature_names.npy', tfidf_feature_names)\n",
        "\n",
        "np.save(path + '/word2vec_train.npy', cbow_embeddings_train)\n",
        "np.save(path + '/word2vec_val.npy', cbow_embeddings_val)\n",
        "np.save(path + '/word2vec_test.npy', cbow_embeddings_test)\n",
        "\n",
        "np.save(path + '/word2vec_smote_tomek_embeddings.npy', train_embeddings_resampled)\n",
        "np.save(path + '/word2vec_smote_tomek_labels.npy', train_labels_resampled)\n",
        "\n",
        "train_lda_df.to_pickle(path + '/lda_train.pkl')\n",
        "val_lda_df.to_pickle(path + '/lda_val.pkl')\n",
        "test_lda_df.to_pickle(path + '/lda_test.pkl')\n",
        "\n",
        "with open(path + '/lda_topics.pkl', 'wb') as f:\n",
        "    pickle.dump(topics, f)"
      ],
      "metadata": {
        "id": "yxSsYeEL5sUv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}